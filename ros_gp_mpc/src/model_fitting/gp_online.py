import torch
import gpytorch
import time
import numpy as np
import copy
from collections import deque
import matplotlib.pyplot as plt

# å¯¼å…¥å¤šè¿›ç¨‹å’Œé˜Ÿåˆ—ç›¸å…³æ¨¡å—
from multiprocessing import Process, Queue, Event
import queue  # ç”¨äºå¤„ç†ç©ºé˜Ÿåˆ—å¼‚å¸¸
import traceback # ç”¨äºæ‰“å°è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯

# =================================================================================
# 1. æ¨¡å‹ä¸æ•°æ®ç¼“å†²åŒºå®šä¹‰
# =================================================================================
class ExactGPModel(gpytorch.models.ExactGP):
    """
    æ ‡å‡†çš„ç²¾ç¡®é«˜æ–¯è¿‡ç¨‹æ¨¡å‹å®šä¹‰ã€‚
    æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå¸¸é‡å‡å€¼å‡½æ•°å’Œä¸€ä¸ªå¸¦æœ‰ç¼©æ”¾æ ¸çš„Maternæ ¸å‡½æ•°ã€‚
    Maternæ ¸é€šå¸¸åœ¨ç‰©ç†ç³»ç»Ÿä¸­æ¯”RBFæ ¸è¡¨ç°å¾—æ›´ç¨³å®šã€‚

    A standard Exact GP model, using a Matern kernel which is often more stable
    for physical systems than an RBF kernel.
    """
    def __init__(self, train_x, train_y, likelihood, ard_num_dims=1):
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(train_x, train_y, likelihood)
        # å®šä¹‰å‡å€¼å‡½æ•°ä¸ºå¸¸é‡
        self.mean_module = gpytorch.means.ConstantMean()
        # å®šä¹‰åæ–¹å·®å‡½æ•°ï¼ˆæ ¸å‡½æ•°ï¼‰
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=ard_num_dims)
        )
    
    # å®šä¹‰æ¨¡å‹çš„å‰å‘ä¼ æ’­
    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        # è¿”å›ä¸€ä¸ªå¤šå…ƒé«˜æ–¯åˆ†å¸ƒ
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

class MultiLevelBuffer:
    """
    å¤šçº§æ•°æ®ç¼“å†²åŒºã€‚
    - è´Ÿè´£é«˜æ•ˆåœ°å­˜å‚¨å’Œé‡‡æ ·æ•°æ®ç‚¹ã€‚
    - æ‹¥æœ‰å¤šä¸ªå±‚çº§ï¼Œæ¯ä¸ªå±‚çº§æœ‰ä¸åŒçš„å®¹é‡å’Œç¨€ç–åº¦ã€‚
    - æ–°æ•°æ®ä¼˜å…ˆè¿›å…¥æœ€é«˜å¯†åº¦å±‚ï¼Œå½“å±‚æ»¡æ—¶ï¼Œæœ€æ—§çš„æ•°æ®ä¼šè¢«â€œæŒ¤å‡ºâ€å¹¶å°è¯•è¿›å…¥ä¸‹ä¸€ç¨€ç–å±‚ã€‚
    """
    def __init__(self, level_max_capacities, level_sparsity_factors):
        # æ£€æŸ¥è¾“å…¥åˆæ³•æ€§
        if len(level_max_capacities) != len(level_sparsity_factors):
            raise ValueError("Capacities and sparsity factor lists must have the same length.")
        self.num_levels = len(level_max_capacities)
        self.capacities = level_max_capacities
        # åˆ›å»ºæ¯ä¸€å±‚çš„åŒç«¯é˜Ÿåˆ—ä½œä¸ºç¼“å†²åŒº
        self.levels = [deque(maxlen=cap) for cap in self.capacities]
        self.sparsity_factors = level_sparsity_factors
        # è®°å½•æ€»å…±æ·»åŠ çš„æ•°æ®ç‚¹æ•°é‡
        self.total_adds = 0

    def insert(self, v_scalar, y_scalar):
        """
        å‘ç¼“å†²åŒºä¸­æ’å…¥ä¸€ä¸ªæ–°çš„æ•°æ®ç‚¹ (v, y)ã€‚
        """
        self.total_adds += 1
        item_to_process = (v_scalar, y_scalar)

        for i in range(self.num_levels):
            if item_to_process is None:
                break

            # æ ¹æ®ç¨€ç–å› å­åˆ¤æ–­å½“å‰å±‚æ˜¯å¦æ¥æ”¶æ­¤æ•°æ®ç‚¹
            if self.sparsity_factors[i] > 0 and self.total_adds % self.sparsity_factors[i] == 0:
                current_level_deque = self.levels[i]
                
                # å¦‚æœç¼“å†²åŒºå·²æ»¡ï¼ŒæŒ¤å‡ºæœ€æ—§çš„æ•°æ®ç‚¹ï¼Œä¼ é€’ç»™ä¸‹ä¸€å±‚å¤„ç†
                next_item_to_process = current_level_deque.popleft() if len(current_level_deque) == current_level_deque.maxlen else None
                current_level_deque.append(item_to_process)
                item_to_process = next_item_to_process
            else:
                # å¦‚æœä¸æ»¡è¶³ç¨€ç–æ¡ä»¶ï¼Œæ­¤æ•°æ®ç‚¹åŸå°ä¸åŠ¨åœ°ä¼ é€’ç»™ä¸‹ä¸€å±‚
                pass

    def get_training_set(self):
        """
        è·å–ç”¨äºè®­ç»ƒçš„å®Œæ•´æ•°æ®é›†ã€‚
        å®ƒä¼šåˆå¹¶æ‰€æœ‰å±‚çº§çš„æ•°æ®å¹¶å»é™¤é‡å¤é¡¹ã€‚
        """
        all_data = []
        for level_deque in self.levels:
            all_data.extend(list(level_deque))
        # ä½¿ç”¨å­—å…¸å»é‡ï¼ŒåŒæ—¶ä¿æŒæ•°æ®ç‚¹çš„æ’å…¥é¡ºåº
        return list(dict.fromkeys(all_data))
    
    def reset(self):
        """æ¸…ç©ºæ‰€æœ‰å†…éƒ¨ç¼“å†²åŒºã€‚"""
        # *** æ ¸å¿ƒä¿®å¤ 2: ä½¿ç”¨æ­£ç¡®çš„å±æ€§å (self.levels) å’Œ (self.capacities) ***
        self.levels = [deque(maxlen=cap) for cap in self.capacities]
        # *** æ ¸å¿ƒä¿®å¤ 3: åŒæ—¶é‡ç½®æ•°æ®ç‚¹è®¡æ•°å™¨ï¼Œç¡®ä¿å®Œå…¨é‡ç½® ***
        self.total_adds = 0
        print("  - MultiLevelBuffer has been reset.")
# =================================================================================
# 2. è®­ç»ƒå†å²è®°å½•å™¨
# =================================================================================
class TrainingHistory:
    """ä¸€ä¸ªç®€å•çš„æ•°æ®ç±»ï¼Œç”¨äºå­˜å‚¨å•æ¬¡è®­ç»ƒä»»åŠ¡ä¸­è¶…å‚æ•°çš„æ¼”åŒ–å†å²ã€‚"""
    def __init__(self):
        self.loss = []
        self.noise = []
        self.lengthscale = []
        self.outputscale = []
        self.learning_rate = []

    def record(self, loss, model, optimizer):
        self.loss.append(loss)
        self.noise.append(model.likelihood.noise.item())
        if isinstance(model.covar_module.base_kernel, gpytorch.kernels.MaternKernel):
            self.lengthscale.append(model.covar_module.base_kernel.lengthscale.item())
        self.outputscale.append(model.covar_module.outputscale.item())
        self.learning_rate.append(optimizer.param_groups[0]['lr'])

    def has_data(self):
        return len(self.loss) > 0

# =================================================================================
# 3. åå°å·¥ä½œè¿›ç¨‹å‡½æ•°
# =================================================================================
def gp_training_worker(
    task_queue: Queue, 
    result_queue: Queue, 
    stop_event, 
    worker_config: dict, 
    dim_idx: int
):
    """
    åå°å·¥ä½œè¿›ç¨‹å‡½æ•°ã€‚
    - è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„è¿›ç¨‹ï¼Œä¸“é—¨ç”¨äºæ‰§è¡Œè€—æ—¶çš„GPè¶…å‚æ•°ä¼˜åŒ–ã€‚
    - å®ƒå¾ªç¯åœ°ä»è‡ªå·±çš„ä»»åŠ¡é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡ï¼Œè®­ç»ƒæ¨¡å‹ï¼Œç„¶åå°†ç»“æœæ”¾å…¥å…¬å…±çš„ç»“æœé˜Ÿåˆ—ã€‚
    """
    device_str = worker_config.get('device_str', 'cpu')
    device = torch.device(device_str)
    
    # åˆå§‹åŒ–ä¸€ä¸ªä»…å±äºæ­¤è¿›ç¨‹çš„æ¨¡å‹ã€ä¼¼ç„¶å’Œè¾¹é™…å¯¹æ•°ä¼¼ç„¶(mll)
    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
    dummy_x = torch.zeros(2, 1, device=device)
    dummy_y = torch.zeros(2, device=device)
    model = ExactGPModel(dummy_x, dummy_y, likelihood, ard_num_dims=1).to(device)
    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

    while not stop_event.is_set():
        try:
            # ä½¿ç”¨è¶…æ—¶æ¥é˜»å¡å¼è·å–ä»»åŠ¡ï¼Œè¿™æ ·å¯ä»¥å‘¨æœŸæ€§åœ°æ£€æŸ¥åœæ­¢äº‹ä»¶
            task = task_queue.get(timeout=0.2) 
            if task is None: # æ”¶åˆ°â€œæ¯’ä¸¸â€(None)ï¼Œè¡¨ç¤ºä¸»è¿›ç¨‹è¦æ±‚é€€å‡º
                break
            
            train_x_tensor, train_y_tensor, initial_state_dict = task
            
            # åŠ è½½æ•°æ®å’Œæ¨¡å‹çŠ¶æ€åˆ°æŒ‡å®šè®¾å¤‡
            train_x_tensor = train_x_tensor.to(device)
            train_y_tensor = train_y_tensor.to(device)
            model.load_state_dict(initial_state_dict['model'])
            likelihood.load_state_dict(initial_state_dict['likelihood'])

            # [å…³é”®æ­¥éª¤] å°†è®­ç»ƒæ•°æ®ä¸æ¨¡å‹å…³è”èµ·æ¥
            model.set_train_data(inputs=train_x_tensor, targets=train_y_tensor, strict=False)
            
            # --- å¼€å§‹æ‰§è¡Œå®Œæ•´çš„ã€é˜»å¡å¼çš„è®­ç»ƒ ---
            model.train()
            likelihood.train()
            optimizer = torch.optim.Adam(model.parameters(), lr=worker_config.get('lr', 0.01))
            history = TrainingHistory()
            
            start_time = time.time()
            for i in range(worker_config.get('n_iter', 100)):
                optimizer.zero_grad()
                output = model(train_x_tensor)
                loss = -mll(output, train_y_tensor)
                loss.backward()
                optimizer.step()
                history.record(loss.item(), model, optimizer)
            duration = time.time() - start_time
            
            # [æœ€ä½³å®è·µ] å°†æ¨¡å‹çŠ¶æ€ç§»å›CPUï¼Œå†æ‰“åŒ…ç»“æœ
            model.cpu()
            likelihood.cpu()
            new_state_dict = {
                'model': model.state_dict(),
                'likelihood': likelihood.state_dict(),
            }
            model.to(device) # ç§»å›å·¥ä½œè®¾å¤‡ï¼Œä»¥å¤‡ä¸‹æ¬¡ä»»åŠ¡

            # å°†å¸¦æœ‰ç»´åº¦æ ‡è¯†ç¬¦çš„ç»“æœæ”¾å…¥å…±äº«çš„ç»“æœé˜Ÿåˆ—
            result_queue.put((dim_idx, new_state_dict, history, duration))

        except queue.Empty:
            # é˜Ÿåˆ—ä¸ºç©ºæ˜¯æ­£å¸¸æƒ…å†µï¼Œç»§ç»­å¾ªç¯
            continue
        except Exception as e:
            print(f"[Worker-{dim_idx}] é”™è¯¯: è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
            traceback.print_exc()

# =================================================================================
# 4. ä¸»è¿›ç¨‹ä¸­çš„GPå®ä¾‹ (çŠ¶æ€æŒæœ‰è€…)
# =================================================================================
class IncrementalGP:
    """
    å¢é‡é«˜æ–¯è¿‡ç¨‹å®ä¾‹ã€‚
    - åœ¨ä¸»è¿›ç¨‹ä¸­è¿è¡Œï¼Œæ¯ä¸ªç»´åº¦ä¸€ä¸ªã€‚
    - å®ƒæœ¬èº«ä¸æ‰§è¡Œè€—æ—¶çš„è®­ç»ƒï¼Œåªè´Ÿè´£æŒæœ‰æœ€æ–°çš„æ¨¡å‹çŠ¶æ€ã€ç®¡ç†æ•°æ®ç¼“å†²åŒºå’Œå½’ä¸€åŒ–ç»Ÿè®¡é‡ã€‚
    """
    def __init__(self, dim_idx, config):
        self.dim_idx = dim_idx
        self.config = config
        self.device = torch.device(config.get('main_process_device', 'cpu'))
        self.epsilon = 1e-7
        
        # æ•°æ®ç¼“å†²åŒº
        self.buffer = MultiLevelBuffer(config['buffer_level_capacities'], config['buffer_level_sparsity'])
        # --- æ–°å¢ï¼šä¸€ä¸ªç”¨äºå­˜å‚¨æ‰€æœ‰å†å²æ•°æ®çš„å®Œæ•´ç¼“å†²åŒº (æ— ç¨€ç–) ---
        # capacities = np.array(config['buffer_level_capacities'])
        # sparsity = np.array(config['buffer_level_sparsity'])
        # max_history_size = int(np.sum(capacities * sparsity))
        # self.full_history_buffer = deque(maxlen=max_history_size)

        # EMA (æŒ‡æ•°ç§»åŠ¨å¹³å‡) å½’ä¸€åŒ–ç»Ÿè®¡é‡
        self.ema_alpha = config.get('ema_alpha', 0.05)
        self.v_mean_ema, self.v_var_ema = 0.0, 1.0
        self.r_mean_ema, self.r_var_ema = 0.0, 1.0
        
        # ä¸»è¿›ç¨‹ä¸­ç”¨äºå¿«é€Ÿé¢„æµ‹çš„â€œå®æ—¶â€æ¨¡å‹
        self.likelihood = gpytorch.likelihoods.GaussianLikelihood().to(self.device)
        dummy_x = torch.zeros(2, 1, device=self.device)
        dummy_y = torch.zeros(2, device=self.device)
        self.model = ExactGPModel(dummy_x, dummy_y, self.likelihood, ard_num_dims=1).to(self.device)
        
        # çŠ¶æ€æ ‡å¿—
        self.updates_since_last_train = 0  # è·ç¦»ä¸Šæ¬¡è®­ç»ƒæœ‰å¤šå°‘æ¬¡æ•°æ®æ›´æ–°
        self.is_trained_once = False       # æ˜¯å¦è‡³å°‘è¢«æˆåŠŸè®­ç»ƒè¿‡ä¸€æ¬¡
        self.is_training_in_progress = False # åå°æ˜¯å¦æ­£åœ¨ä¸ºæ­¤ç»´åº¦è¿›è¡Œè®­ç»ƒ
        self.last_training_history = TrainingHistory() # ä¿å­˜æœ€è¿‘ä¸€æ¬¡çš„è®­ç»ƒå†å²

    def add_data_point(self, x, y):
        # --- ä¿®æ”¹ï¼šåŒæ—¶å‘ä¸¤ä¸ªç¼“å†²åŒºæ·»åŠ æ•°æ® ---
        # self.full_history_buffer.append((x, y)) # 1. è®°å½•åˆ°å®Œæ•´å†å²ä¸­
        self.buffer.insert(x, y)
        self.updates_since_last_train += 1
    
    def get_and_normalize_data(self):
        """è·å–å¹¶å½’ä¸€åŒ–æ‰€æœ‰æ•°æ®ï¼Œç”¨äºæ´¾å‘ç»™åå°è®­ç»ƒå’Œæ›´æ–°ä¸»æ¨¡å‹ã€‚"""
        training_data_raw = self.buffer.get_training_set()
        if not training_data_raw or len(training_data_raw) < 2: return None, None
        
        raw_v = np.array([p[0] for p in training_data_raw], dtype=np.float32)
        raw_r = np.array([p[1] for p in training_data_raw], dtype=np.float32)
        
        # æ›´æ–°EMAç»Ÿè®¡é‡
        num_points = len(raw_v)
        if num_points < self.config.get('min_points_for_ema', 30):
            v_mean, v_var = np.mean(raw_v), np.var(raw_v)
            r_mean, r_var = np.mean(raw_r), np.var(raw_r)
        else:
            self.v_mean_ema = (1 - self.ema_alpha) * self.v_mean_ema + self.ema_alpha * np.mean(raw_v)
            self.v_var_ema = (1 - self.ema_alpha) * self.v_var_ema + self.ema_alpha * np.var(raw_v)
            self.r_mean_ema = (1 - self.ema_alpha) * self.r_mean_ema + self.ema_alpha * np.mean(raw_r)
            self.r_var_ema = (1 - self.ema_alpha) * self.r_var_ema + self.ema_alpha * np.var(raw_r)
            v_mean, v_var = self.v_mean_ema, self.v_var_ema
            r_mean, r_var = self.r_mean_ema, self.r_var_ema
        
        # ä½¿ç”¨ç»Ÿè®¡é‡è¿›è¡Œå½’ä¸€åŒ–
        train_x_norm = torch.tensor((raw_v - v_mean) / np.sqrt(v_var + self.epsilon), device=self.device).view(-1, 1)
        train_y_norm = torch.tensor((raw_r - r_mean) / np.sqrt(r_var + self.epsilon), device=self.device)
        return train_x_norm, train_y_norm

    def get_current_state_for_worker(self):
        """è·å–å½“å‰æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼Œå‡†å¤‡å‘é€ç»™workerã€‚"""
        self.model.cpu()
        self.likelihood.cpu()
        state = {'model': self.model.state_dict(), 'likelihood': self.likelihood.state_dict()}
        self.model.to(self.device)
        return state

    def load_new_state_from_worker(self, new_state_dict, history):
        """ä»workeråŠ è½½è®­ç»ƒå¥½çš„æ–°çŠ¶æ€å­—å…¸å¹¶æ›´æ–°å®æ—¶æ¨¡å‹ã€‚"""
        self.model.load_state_dict(new_state_dict['model'])
        self.likelihood.load_state_dict(new_state_dict['likelihood'])
        self.model.to(self.device)
        
        # [å…³é”®æ­¥éª¤] åŠ è½½æ–°è¶…å‚æ•°åï¼Œå¿…é¡»ç«‹å³ç”¨æœ€æ–°çš„å…¨é‡æ•°æ®æ›´æ–°æ¨¡å‹ï¼Œå¦åˆ™é¢„æµ‹ä¼šä½¿ç”¨æ—§æ•°æ®
        train_x_norm, train_y_norm = self.get_and_normalize_data()
        if train_x_norm is not None:
             self.model.set_train_data(train_x_norm, train_y_norm, strict=False)

        # æ›´æ–°çŠ¶æ€æ ‡å¿—
        self.last_training_history = history
        self.is_training_in_progress = False
        self.updates_since_last_train = 0
        if not self.is_trained_once: self.is_trained_once = True
    
    def reset(self):
        """å°†æ­¤GPå®ä¾‹å®Œå…¨é‡ç½®åˆ°å…¶åˆå§‹çŠ¶æ€ã€‚"""
        # 1. é‡ç½®æ•°æ®ç¼“å†²åŒº
        self.buffer.reset()

        # 2. é‡ç½®æ‰€æœ‰çŠ¶æ€æ ‡å¿—
        self.is_trained_once = False
        self.is_training_in_progress = False
        self.updates_since_last_train = 0
        # ä¿®å¤: é‡ç½®ä¸ºä¸€ä¸ªæ–°çš„ç©ºå†å²å¯¹è±¡ï¼Œè€Œä¸æ˜¯None
        self.last_training_history = TrainingHistory()

        # 3. é‡ç½®å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ® (EMA)
        self.v_mean_ema = 0.0
        self.v_var_ema = 1.0
        self.r_mean_ema = 0.0
        self.r_var_ema = 1.0
        # self.ema_counter = 0 # ç§»é™¤æœªä½¿ç”¨çš„å˜é‡

        # 4. *** æ ¸å¿ƒä¿®å¤: ä»¥æ­£ç¡®çš„é¡ºåºé‡æ–°åˆå§‹åŒ–æ¨¡å‹å’Œä¼¼ç„¶ ***
        #    å¿…é¡»å…ˆåˆ›å»ºæ–°çš„likelihoodï¼Œå†ç”¨å®ƒæ¥åˆ›å»ºæ–°çš„modelã€‚
        self.likelihood = gpytorch.likelihoods.GaussianLikelihood().to(self.device)
        self.model = ExactGPModel(None, None, self.likelihood).to(self.device)
        
        print(f"  - IncrementalGP for Dim has been fully reset.")

# =================================================================================
# 5. ç®¡ç†å™¨ï¼šè´Ÿè´£ç¼–æ’æ‰€æœ‰ç»„ä»¶
# =================================================================================
class IncrementalGPManager:
    """
    åœ¨çº¿é«˜æ–¯è¿‡ç¨‹ç®¡ç†å™¨ã€‚
    - è´Ÿè´£åˆ›å»ºå’Œç®¡ç†æ‰€æœ‰åå°å·¥ä½œè¿›ç¨‹å’Œé€šä¿¡é˜Ÿåˆ—ã€‚
    - æä¾›ç»Ÿä¸€çš„æ¥å£ (`update`, `predict`, `shutdown`) ç»™å¤–éƒ¨è°ƒç”¨ã€‚
    """
    def __init__(self, config):
        self.config = config
        self.num_dimensions = config.get('num_dimensions', 3)
        self.gps = [IncrementalGP(i, config) for i in range(self.num_dimensions)]
        
        # ä¸ºæ¯ä¸ªworkeråˆ›å»ºä¸€ä¸ªä¸“å±çš„ä»»åŠ¡é˜Ÿåˆ—
        self.task_queues = [Queue() for _ in range(self.num_dimensions)]
        # æ‰€æœ‰workerå…±äº«ä¸€ä¸ªå…¬å…±çš„ç»“æœé˜Ÿåˆ—
        self.result_queue = Queue()
        # ç”¨äºé€šçŸ¥æ‰€æœ‰workeråœæ­¢çš„äº‹ä»¶
        self.stop_event = Event()
        self.workers = []
        
        # æ–°å¢: ç”¨äºå­˜å‚¨åå°è®­ç»ƒè€—æ—¶çš„åˆ—è¡¨
        self.training_durations = []
        
        print(f"[ç®¡ç†å™¨] åˆå§‹åŒ–... æ­£åœ¨ä¸º {self.num_dimensions} ä¸ªç»´åº¦å¯åŠ¨åå°å·¥ä½œè¿›ç¨‹ã€‚")
        for i in range(self.num_dimensions):
            worker_config = {
                'n_iter': config.get('worker_train_iters', 150),
                'lr': config.get('worker_lr', 0.01),
                'device_str': config.get('worker_device_str', 'cpu'),
            }
            worker = Process(target=gp_training_worker, args=(self.task_queues[i], self.result_queue, self.stop_event, worker_config, i))
            worker.daemon = True # è®¾ç½®ä¸ºå®ˆæŠ¤è¿›ç¨‹ï¼Œä¸»è¿›ç¨‹é€€å‡ºæ—¶å®ƒä¼šè‡ªåŠ¨ç»ˆæ­¢
            worker.start()
            self.workers.append(worker)
        print("[ç®¡ç†å™¨] æ‰€æœ‰åå°å·¥ä½œè¿›ç¨‹å·²æˆåŠŸå¯åŠ¨ã€‚")

    def update(self, new_velocities, new_residuals):
        """éé˜»å¡æ›´æ–°ï¼šæ·»åŠ æ•°æ®ç‚¹ï¼Œå¹¶æ ¹æ®æ¡ä»¶è§¦å‘åå°è®­ç»ƒä»»åŠ¡ã€‚"""
        for i in range(self.num_dimensions):
            gp = self.gps[i]
            gp.add_data_point(new_velocities[i], new_residuals[i])
            
            if gp.is_training_in_progress: 
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³è§¦å‘è®­ç»ƒçš„æ¡ä»¶
            num_training_points = len(gp.buffer.get_training_set())
            should_trigger = False
            # æ¡ä»¶1: é¦–æ¬¡è®­ç»ƒ
            if not gp.is_trained_once and num_training_points >= self.config.get('min_points_for_initial_train', 50):
                should_trigger = True
            # æ¡ä»¶2: å®šæœŸå†è®­ç»ƒ
            elif gp.is_trained_once and gp.updates_since_last_train >= self.config.get('refit_hyperparams_interval', 100):
                should_trigger = True
            
            if should_trigger:
                train_x, train_y = gp.get_and_normalize_data()
                if train_x is not None:
                    print(f"ğŸ§  [ç®¡ç†å™¨] Dim-{i}: æ»¡è¶³è®­ç»ƒæ¡ä»¶ ({num_training_points}ä¸ªç‚¹), æ­£åœ¨æ´¾å‘ä»»åŠ¡è‡³åå°...")
                    current_state = gp.get_current_state_for_worker()
                    task = (train_x, train_y, current_state)
                    self.task_queues[i].put(task)
                    gp.is_training_in_progress = True

    def poll_for_results(self):
        """éé˜»å¡åœ°æ£€æŸ¥å¹¶åº”ç”¨å·²å®Œæˆçš„è®­ç»ƒç»“æœã€‚"""
        try:
            dim_idx, new_state_dict, history, duration = self.result_queue.get_nowait()
            print(f"ğŸ‰ [ç®¡ç†å™¨] æ”¶åˆ° Worker-{dim_idx} çš„è®­ç»ƒç»“æœï¼è€—æ—¶: {duration:.2f}sã€‚æ­£åœ¨æ›´æ–°å®æ—¶æ¨¡å‹...")
            # æ–°å¢: è®°å½•è®­ç»ƒæ—¶é•¿
            self.training_durations.append(duration)
            self.gps[dim_idx].load_new_state_from_worker(new_state_dict, history)
        except queue.Empty:
            pass # é˜Ÿåˆ—ä¸ºç©ºæ˜¯æ­£å¸¸æƒ…å†µ

    def shutdown(self):
        """ä¼˜é›…åœ°å…³é—­æ‰€æœ‰åå°å·¥ä½œè¿›ç¨‹ã€‚"""
        print("\n gracefully shutting down all background worker processes...")
        self.stop_event.set()
        for q in self.task_queues:
            q.put(None)
            
        time.sleep(0.5)
        for i, worker in enumerate(self.workers):
            worker.join(timeout=2.0)
            if worker.is_alive():
                print(f"[ç®¡ç†å™¨] Worker-{i} æœªèƒ½æ­£å¸¸å…³é—­ï¼Œå°†å¼ºåˆ¶ç»ˆæ­¢ã€‚")
                worker.terminate()
        print("[ç®¡ç†å™¨] æ‰€æœ‰åå°å·¥ä½œè¿›ç¨‹å·²æˆåŠŸå…³é—­ã€‚")
    
    def _clear_queue(self, q):
        """å®‰å…¨åœ°æ¸…ç©ºä¸€ä¸ªå¤šè¿›ç¨‹é˜Ÿåˆ—ã€‚"""
        try:
            while True:
                q.get_nowait()
        except queue.Empty:
            pass

    def reset(self):
       """
       é‡ç½®ç®¡ç†å™¨åŠå…¶æ‰€æœ‰å†…éƒ¨GPå®ä¾‹çš„çŠ¶æ€ï¼Œä¸ºä¸€æ¬¡æ–°çš„ç‹¬ç«‹å®éªŒè¿è¡Œåšå‡†å¤‡ã€‚
       è¿™ä¼šæ¸…ç©ºæ‰€æœ‰æ•°æ®ç¼“å†²åŒºã€é‡ç½®æ¨¡å‹ã€å¹¶æ¸…ç©ºæ‰€æœ‰é€šä¿¡é˜Ÿåˆ—ã€‚
       """
       
       print("\nğŸ”„ Resetting IncrementalGPManager state for new experiment run...")
       
       # 1. å§”æ‰˜æ¯ä¸ªGPå®ä¾‹è¿›è¡Œé‡ç½® (æ¸…ç©ºç¼“å†²åŒº, é‡ç½®æ¨¡å‹å’ŒçŠ¶æ€)
       for gp in self.gps:
           gp.reset()
           gp.is_trained_once = False
           
       # 2. *** æ ¸å¿ƒä¿®å¤: æ¸…ç©ºæ‰€æœ‰ä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœé˜Ÿåˆ— ***
       print("  - Clearing communication queues...")
       for q in self.task_queues:
           self._clear_queue(q)
       self._clear_queue(self.result_queue)
       
       # 3. *** æ ¸å¿ƒä¿®å¤: é‡ç½®æ€§èƒ½ç»Ÿè®¡åˆ—è¡¨ ***
       self.training_durations = []
       
       print("âœ… Manager reset complete.") 

    def predict(self, query_velocities):
        """ä½¿ç”¨ä¸»è¿›ç¨‹ä¸­çš„å®æ—¶æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚"""
        means = np.zeros((query_velocities.shape[0], self.num_dimensions), dtype=float)
        variances = np.ones_like(means, dtype=float)

        for i, gp in enumerate(self.gps):
            if not gp.is_trained_once: continue
            
            v_query = np.atleast_1d(query_velocities[:, i])
            if v_query.size == 0: continue
            
            v_std_ema = np.sqrt(gp.v_var_ema + gp.epsilon)
            v_query_norm = (v_query - gp.v_mean_ema) / v_std_ema
            # æœ€ç»ˆä¿®å¤ï¼šé€šè¿‡è®¿é—®æ¨¡å‹çš„ä¸€ä¸ªå‚æ•°æ¥å®‰å…¨åœ°è·å–å…¶dtype
            model_dtype = next(gp.model.parameters()).dtype
            v_query_torch = torch.tensor(v_query_norm, device=gp.device, dtype=model_dtype).view(-1, 1)
            
            gp.model.eval()
            gp.likelihood.eval()
            with torch.no_grad(), gpytorch.settings.fast_pred_var():
                preds = gp.likelihood(gp.model(v_query_torch))
                mean_norm = preds.mean.cpu().numpy()
                var_norm = preds.variance.cpu().numpy()
            
            r_std_ema = np.sqrt(gp.r_var_ema + gp.epsilon)
            means[:, i] = mean_norm * r_std_ema + gp.r_mean_ema
            variances[:, i] = var_norm * (gp.r_var_ema + gp.epsilon)
            
        return means, np.maximum(variances, 1e-9)

    # =================================================================================
    # å¯è§†åŒ–éƒ¨åˆ† (ç¾åŒ–å’Œæ”¹è¿›)
    # =================================================================================
    def visualize_all_dims(self, sim_velocities, sim_residuals):
        """å¯è§†åŒ–æ‰€æœ‰ç»´åº¦çš„æœ€ç»ˆæ‹Ÿåˆç»“æœã€‚"""
        if not any(gp.is_trained_once for gp in self.gps):
            print("[å¯è§†åŒ–] æ— æ³•ç»˜åˆ¶æ‹Ÿåˆå›¾ï¼Œæ²¡æœ‰ä»»ä½•GPè¢«è®­ç»ƒè¿‡ã€‚")
            return
        
        # ä¿®å¤: ä½¿ç”¨å…¼å®¹æ€§æ›´å¥½çš„æ ·å¼å
        try:
            plt.style.use('seaborn-whitegrid')
        except IOError:
            print("âš ï¸ [å¯è§†åŒ–] 'seaborn-whitegrid' style not found. Using default style.")
            plt.style.use('default')

        fig, axes = plt.subplots(1, self.num_dimensions, figsize=(24, 7), dpi=120)
        # è‹±æ–‡æ€»æ ‡é¢˜
        fig.suptitle("Asynchronous GP Regression - Final Fit", fontsize=20, weight='bold')

        for i, (ax, gp) in enumerate(zip(axes, self.gps)):
            if not gp.is_trained_once: 
                ax.set_title(f'Dimension {i} - Not Trained Yet', style='italic')
                continue
            
            v_all_data = sim_velocities[:, i]
            r_all_data = sim_residuals[:, i]
            # ç»˜åˆ¶æ‰€æœ‰å†å²æ•°æ®ç‚¹ä½œä¸ºèƒŒæ™¯
            ax.plot(v_all_data, r_all_data, 'o', color='lightgrey', markersize=3, alpha=0.5, label='Full History Data')

            # è·å–å¹¶çªå‡ºæ˜¾ç¤ºå½“å‰ç¼“å†²åŒºä¸­çš„æ•°æ®ç‚¹
            current_buffer_data = gp.buffer.get_training_set()
            if current_buffer_data:
                v_buffer = [p[0] for p in current_buffer_data]
                r_buffer = [p[1] for p in current_buffer_data]
                ax.plot(v_buffer, r_buffer, 'o', color='#3498db', markersize=4, label='Active Buffer Data')

            # å‡†å¤‡ç”¨äºç»˜åˆ¶GPé¢„æµ‹æ›²çº¿çš„æŸ¥è¯¢ç‚¹
            v_range = np.linspace(v_all_data.min(), v_all_data.max(), 200)
            query_points = np.zeros((200, self.num_dimensions))
            for j in range(self.num_dimensions):
                query_points[:, j] = v_range if i == j else self.gps[j].v_mean_ema

            pred_mean, pred_var = self.predict(query_points)
            pred_std = np.sqrt(pred_var[:, i])
            
            # ç»˜åˆ¶GPå‡å€¼é¢„æµ‹å’Œç½®ä¿¡åŒºé—´
            ax.plot(v_range, pred_mean[:, i], color='#e74c3c', linestyle='-', linewidth=2.5, label='GP Mean Prediction')
            ax.fill_between(
                v_range,
                pred_mean[:, i] - 1.96 * pred_std,
                pred_mean[:, i] + 1.96 * pred_std,
                color='#e74c3c', alpha=0.2, label='95% Confidence Interval'
            )
            # è®¾ç½®è‹±æ–‡æ ‡é¢˜å’Œåæ ‡è½´
            ax.set_title(f'Dimension {i} Fit', fontsize=16)
            ax.set_xlabel(f'Input: Velocity Dim {i} (m/s)', fontsize=12)
            ax.set_ylabel(f'Output: Residual Dim {i} (m/s^2)', fontsize=12)
            ax.legend()
            ax.set_xlim(v_all_data.min(), v_all_data.max())

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.show(block=False)

    def visualize_training_history(self):
        """å¯è§†åŒ–æœ€è¿‘ä¸€æ¬¡å®Œæˆçš„è®­ç»ƒä»»åŠ¡ä¸­çš„è¶…å‚æ•°æ¼”åŒ–ã€‚"""
        if not any(gp.last_training_history.has_data() for gp in self.gps):
            print("[å¯è§†åŒ–] æ— æ³•ç»˜åˆ¶è®­ç»ƒå†å²ï¼Œè¿˜æ²¡æœ‰ä»»ä½•è®­ç»ƒå®Œæˆã€‚")
            return
        
        # ä¿®å¤: ä½¿ç”¨å…¼å®¹æ€§æ›´å¥½çš„æ ·å¼å
        try:
            plt.style.use('seaborn-whitegrid')
        except IOError:
            print("[å¯è§†åŒ–] 'seaborn-whitegrid' style not found. Using default style.")
            plt.style.use('default')
            
        fig, axes = plt.subplots(self.num_dimensions, 4, figsize=(20, 4 * self.num_dimensions), sharex=True, squeeze=False, dpi=120)
        fig.suptitle("Hyperparameter Evolution - Last Completed Training Task", fontsize=20, weight='bold')

        for i, gp in enumerate(self.gps):
            history = gp.last_training_history
            if not history.has_data():
                for j in range(4):
                   axes[i, j].text(0.5, 0.5, 'Not Trained Yet', ha='center', va='center', style='italic')
                   axes[i, j].set_title(f"Dim {i} - No History")
                continue
            
            iters = range(len(history.loss))
            param_names = ['Loss', 'Likelihood Noise', 'Kernel Lengthscale', 'Kernel Outputscale']
            data_to_plot = [history.loss, history.noise, history.lengthscale, history.outputscale]

            for j, (name, data) in enumerate(zip(param_names, data_to_plot)):
                ax = axes[i, j]
                ax.plot(iters, data, marker='.', linestyle='-', markersize=4)
                ax.set_title(f"Dim {i}: {name}", fontsize=14)
                ax.grid(True, linestyle='--', alpha=0.6)
                if i == self.num_dimensions - 1:
                    ax.set_xlabel("Optimization Iteration", fontsize=12)
        
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.show(block=False)

# =================================================================================
#  6. ç¤ºä¾‹è¿è¡Œ
# =================================================================================
if __name__ == '__main__':
    import multiprocessing
    try:
        multiprocessing.set_start_method("spawn", force=True)
    except (RuntimeError, ValueError):
        pass # åœ¨éLinuxç³»ç»Ÿä¸Šæˆ–ç‰¹å®šç¯å¢ƒä¸­å¯èƒ½ä¼šå¤±è´¥ï¼Œå±äºæ­£å¸¸æƒ…å†µ

    # å®šä¹‰GPç®¡ç†å™¨å’Œä»¿çœŸçš„é…ç½®
    final_config = {
        'num_dimensions': 3,
        'main_process_device': 'cuda',
        'worker_device_str': 'cuda',
        'buffer_level_capacities': [10, 15, 20], # ä¸‰å±‚ç¼“å†²åŒºå®¹é‡
        'buffer_level_sparsity': [1, 2, 5],      # ç¨€ç–å› å­ï¼šæ¯1/2/5ä¸ªç‚¹å­˜å…¥
        'min_points_for_initial_train': 30,      # è§¦å‘é¦–æ¬¡è®­ç»ƒçš„æœ€å°æ•°æ®ç‚¹
        'min_points_for_ema': 30,                # å¯ç”¨EMAæ‰€éœ€çš„æœ€å°æ•°æ®ç‚¹
        'refit_hyperparams_interval': 20,       # è§¦å‘å†è®­ç»ƒçš„æ›´æ–°æ¬¡æ•°é—´éš”
        'worker_train_iters': 70,               # åå°è®­ç»ƒè¿­ä»£æ¬¡æ•°
        'worker_lr': 0.03,                       # è®­ç»ƒå­¦ä¹ ç‡
        'ema_alpha': 0.05,                       # EMAå¹³æ»‘ç³»æ•°
    }
    
    manager = IncrementalGPManager(config=final_config)
    
    # --- ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ® ---
    num_total_samples = 1200
    time_signal = np.linspace(0, 120, num_total_samples)
    sim_velocities = np.zeros((num_total_samples, manager.num_dimensions))
    sim_residuals = np.zeros((num_total_samples, manager.num_dimensions))
    
    sim_velocities[:, 0] = 1.0 * np.sin(0.2 * time_signal) + 0.5 * np.cos(0.5 * time_signal + 0.5)
    sim_residuals[:, 0] = (0.3 * np.sin(2.5 * sim_velocities[:, 0]) + 0.05 * sim_velocities[:, 0] + 0.02 * np.random.randn(num_total_samples))
    sim_velocities[:, 1] = 0.8 * np.sin(0.25 * time_signal + 1.0) + 0.3 * np.sin(0.6 * time_signal)
    sim_residuals[:, 1] = (0.4 * np.exp(-15.0 * (sim_velocities[:, 1] - 0.5)**2) + 0.2 * np.cos(3.0 * sim_velocities[:, 1]) + 0.03 * np.random.randn(num_total_samples))
    sim_velocities[:, 2] = 0.6 * np.sin(0.3 * time_signal + 2.0) - 0.2 * (time_signal / 60)
    sim_residuals[:, 2] = (0.2 * sim_velocities[:, 2]**2 - 0.1 * sim_velocities[:, 2] + 0.05 + 0.025 * np.random.randn(num_total_samples))

    # --- è¿è¡Œä¸»ä»¿çœŸå¾ªç¯ ---
    print("\n" + "="*50)
    print("å¼€å§‹è¿è¡Œå¼‚æ­¥è®­ç»ƒä»¿çœŸ...")
    print("="*50)
    main_loop_times = []
    
    try:
        for i in range(num_total_samples):
            start_t = time.perf_counter()
            
            # 1. (éé˜»å¡) æ´¾å‘æ–°æ•°æ®å’Œå¯èƒ½çš„è®­ç»ƒä»»åŠ¡
            manager.update(sim_velocities[i, :], sim_residuals[i, :])
            
            # 2. (éé˜»å¡) æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„ç»“æœéœ€è¦åº”ç”¨
            manager.poll_for_results()
            
            main_loop_times.append((time.perf_counter() - start_t) * 1000)

            # 3. æ¨¡æ‹Ÿæ§åˆ¶å‘¨æœŸçš„å…¶ä½™å·¥ä½œè´Ÿè½½ (ä¾‹å¦‚ï¼Œ10ms)
            time.sleep(0.01)
            
            if (i + 1) % 200 == 0:
                print(f"  [è¿›åº¦] ä»¿çœŸæ­¥éª¤ {i+1}/{num_total_samples} å·²å¤„ç†...")

    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ä»¿çœŸã€‚")
    finally:
        # ç¡®ä¿æ— è®ºå¦‚ä½•éƒ½ä¼˜é›…åœ°å…³é—­åå°è¿›ç¨‹
        manager.shutdown()

    # --- æ€§èƒ½ç»Ÿè®¡ä¸å¯è§†åŒ– ---
    main_loop_times = np.array(main_loop_times)
    print("\n" + "="*50)
    print("ä¸»å¾ªç¯æ€§èƒ½ç»Ÿè®¡ (ä¸å«åå°è®­ç»ƒè€—æ—¶)")
    print("="*50)
    print(f"  - å¹³å‡å¾ªç¯è€—æ—¶: {np.mean(main_loop_times):.4f} ms")
    print(f"  - ä¸­ä½æ•°è€—æ—¶:   {np.median(main_loop_times):.4f} ms")
    print(f"  - æœ€å¤§å¾ªç¯è€—æ—¶:   {np.max(main_loop_times):.2f} ms")
    print(f"  - 99ç™¾åˆ†ä½è€—æ—¶: {np.percentile(main_loop_times, 99):.2f} ms")
    
    # æ–°å¢: åå°è®­ç»ƒè€—æ—¶ç»Ÿè®¡
    if manager.training_durations:
        durations = np.array(manager.training_durations)
        print("\n" + "="*50)
        print("åå°è®­ç»ƒä»»åŠ¡è€—æ—¶ç»Ÿè®¡")
        print("="*50)
        print(f"  - å®Œæˆçš„è®­ç»ƒä»»åŠ¡æ€»æ•°: {len(durations)}")
        print(f"  - å¹³å‡è®­ç»ƒè€—æ—¶: {np.mean(durations):.2f} s")
        print(f"  - æœ€çŸ­è®­ç»ƒè€—æ—¶: {np.min(durations):.2f} s")
        print(f"  - æœ€é•¿è®­ç»ƒè€—æ—¶: {np.max(durations):.2f} s")


    print("\n" + "="*50)
    print("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print("="*50)
    manager.visualize_all_dims(sim_velocities, sim_residuals)
    manager.visualize_training_history()
    
    # ç»˜åˆ¶ä¸»å¾ªç¯æ—¶é—´åˆ†å¸ƒå›¾
    # ä¿®å¤: ä½¿ç”¨å…¼å®¹æ€§æ›´å¥½çš„æ ·å¼å
    try:
        plt.style.use('seaborn-whitegrid')
    except IOError:
        plt.style.use('default')
        
    plt.figure(figsize=(12, 7), dpi=100)
    plt.hist(main_loop_times, bins=50, alpha=0.75, color='#3498db', label='Update Times')
    plt.axvline(np.mean(main_loop_times), color='#e74c3c', linestyle='--', linewidth=2, label=f'Mean: {np.mean(main_loop_times):.2f} ms')
    plt.axvline(np.median(main_loop_times), color='#f1c40f', linestyle=':', linewidth=2.5, label=f'Median: {np.median(main_loop_times):.2f} ms')
    plt.xlabel("Main Loop Update Time (ms)", fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.title("Distribution of Main Loop Update Times", fontsize=16, weight='bold')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.show()

